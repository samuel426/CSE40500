## NPU, ONNX Runtime(GPU), TensorRT(GPU) 성능 비교 분석

제공된 3개의 JSON 파일 (npu_performance_20250528_132327.json, onnx_performance_20250529_134235.json, tensorrt_performance_20250529_135451.json)을 기반으로 NPU(Hailo-8L), ONNX Runtime(GPU), TensorRT(GPU) 환경에서의 모델 성능을 비교 분석합니다.

공통적으로 존재하는 `GRU_Apple` 모델의 측정 결과를 기준으로 비교하겠습니다.

---

### 1. Latency (지연 시간, 단위: ms) - `GRU_Apple` 모델 기준

| 구분     | NPU (Hailo-8L) | ONNX Runtime (GPU) | TensorRT (GPU) | TensorRT 우위 (NPU 대비) | TensorRT 우위 (ONNX 대비) |
| :------- | :------------- | :----------------- | :------------- | :----------------------- | :------------------------ |
| **mean** | 1.326          | 0.209              | 0.085          | **약 15.6배 빠름**       | **약 2.46배 빠름**        |
| **median**| 1.319          | 0.203              | 0.072          | **약 18.3배 빠름**       | **약 2.82배 빠름**        |
| **min**  | 1.278          | 0.176              | 0.067          | **약 19.1배 빠름**       | **약 2.63배 빠름**        |
| **max**  | 1.417          | 0.355              | 0.703          | **약 2.0배 빠름**        | ONNX Runtime이 더 안정적  |
| **std**  | 0.031          | 0.028              | 0.075          | ONNX Runtime이 더 안정적  | ONNX Runtime이 더 안정적  |
| **p95**  | 1.397          | 0.262              | 0.082          | **약 17.0배 빠름**       | **약 3.20배 빠름**        |
| **p99**  | 1.413          | 0.323              | 0.396          | **약 3.6배 빠름**        | ONNX Runtime이 더 안정적  |

*   **분석:**
    *   **평균(mean), 중앙값(median), 최소(min) 및 p95 지연 시간**에서 TensorRT가 세 플랫폼 중 가장 우수한 성능을 보입니다. NPU 대비 약 15~19배, ONNX Runtime 대비 약 2.4~3.2배 더 빠릅니다.
    *   ONNX Runtime 역시 NPU에 비해 평균적으로 약 6.3배 빠른 지연 시간을 제공합니다.
    *   **최대(max) 지연 시간**의 경우 NPU가 가장 안정적이었고, 그 다음이 ONNX Runtime, TensorRT 순이었습니다. TensorRT는 평균 성능은 매우 우수하지만, 일부 실행에서 상대적으로 큰 지연이 발생한 것으로 보입니다.
    *   **표준편차(std)**는 ONNX Runtime이 가장 작아 지연 시간의 변동성이 가장 적었고, 그 다음이 NPU, TensorRT 순이었습니다.

---

### 2. Throughput (초당 처리량, 단위: FPS) - `GRU_Apple` 모델 기준

| 구분             | NPU (Hailo-8L) | ONNX Runtime (GPU) | TensorRT (GPU) | TensorRT 우위 (NPU 대비) | TensorRT 우위 (ONNX 대비) |
| :--------------- | :------------- | :----------------- | :------------- | :----------------------- | :------------------------ |
| **throughput_fps** | 764.05         | 4792.56            | 11725.43       | **약 15.3배 높음**       | **약 2.45배 높음**        |

*   **분석:**
    *   Throughput은 Latency와 반비례 관계이므로, TensorRT가 가장 높은 처리량을 보였습니다. NPU 대비 약 15.3배, ONNX Runtime 대비 약 2.45배 더 많은 추론을 초당 처리할 수 있습니다.
    *   ONNX Runtime은 NPU 대비 약 6.3배 높은 처리량을 나타냅니다.
    *   NPU JSON 파일의 `memory.avg_model_fps` (29.03 FPS)는 별도의 모니터링 중 측정된 평균 FPS로, 레이턴시 기반의 전체 `throughput_fps`와는 다른 지표이므로 직접 비교에는 주의가 필요합니다. 여기서는 레이턴시 기반 `throughput_fps`를 사용했습니다.

---

### 3. RMSE (Root Mean Squared Error, 예측 오차) - `GRU_Apple` 모델 기준

| 구분   | NPU (Hailo-8L) | ONNX Runtime (GPU) | TensorRT (GPU) | 비고 (낮을수록 좋음) |
| :----- | :------------- | :----------------- | :------------- | :------------------- |
| **rmse** | 54.9047        | 52.1410            | 52.1410        | GPU 실행 시 소폭 우수  |

*   **분석:**
    *   RMSE 값은 ONNX Runtime과 TensorRT가 거의 동일하게 나타났으며 (소수점 넷째 자리까지 동일), NPU보다 약간 낮은 오차율을 보였습니다. 이는 GPU 환경에서의 부동소수점 연산 정밀도 또는 모델 변환/최적화 과정의 차이에서 기인할 수 있습니다. 세 플랫폼 모두 유사한 수준의 예측 정확도를 유지하고 있다고 볼 수 있습니다.

---

### 4. Utilization (평균 사용률, %) - `GRU_Apple` 모델 기준

| 구분                      | NPU (Hailo-8L) | ONNX Runtime (GPU) | TensorRT (GPU) | 비고                                                                 |
| :------------------------ | :------------- | :----------------- | :------------- | :------------------------------------------------------------------- |
| **avg_device_utilization**| 14.10%         | 0.0%               | 0.0%           | NPU는 일정 수준 사용, GPU는 해당 모델에 대해 부하 거의 없음.             |
| **avg_model_utilization** | 14.10%         | 0.0%               | 0.0%           | `avg_device_utilization`과 동일 (NPU는 device=model, GPU는 전체 GPU 사용률) |

*   **분석:**
    *   NPU는 `GRU_Apple` 모델 실행 시 평균 약 14.1%의 사용률을 보였습니다.
    *   ONNX Runtime과 TensorRT 환경에서는 GPU 사용률이 0.0%로 기록되었습니다. 이는 테스트된 `GRU_Apple` 모델이 사용된 GPU(RTX 3070 Ti로 추정)의 전체 연산 능력에 비해 매우 가벼워, 추론 중 거의 부하를 유발하지 않았음을 의미합니다.
    *   TensorRT 결과에서 `utilization_samples`가 1로 나온 것은, 이전 분석에서 언급된 바와 같이, 모니터링 스레드가 아닌 초기 스냅샷 값만 사용되었을 가능성을 시사합니다. ONNX Runtime은 13개의 샘플을 수집했습니다.

---

### 종합 결론

*   **성능 (Latency & Throughput):**
    *   **TensorRT(GPU)**가 세 플랫폼 중 압도적으로 가장 우수한 추론 성능(가장 낮은 지연 시간 및 가장 높은 처리량)을 제공합니다.
    *   **ONNX Runtime(GPU)**도 NPU에 비해 훨씬 뛰어난 성능을 보여줍니다.
    *   **NPU(Hailo-8L)**는 엣지 디바이스로서 전력 효율성에 중점을 둔 하드웨어이므로, 고성능 GPU와의 절대적인 속도 비교에서는 불리하지만, 특정 엣지 환경에서는 충분한 성능을 제공할 수 있습니다.

*   **정확도 (RMSE):**
    *   세 플랫폼 모두 `GRU_Apple` 모델에 대해 유사한 수준의 RMSE를 보였으며, GPU 환경(ONNX, TensorRT)에서 미세하게 더 낮은 오차율을 나타냈습니다.

*   **자원 사용률:**
    *   NPU는 모델 실행 중 일정 수준의 사용률을 보인 반면, 고성능 GPU 환경에서는 `GRU_Apple`과 같은 모델이 GPU 자원을 거의 활용하지 못할 정도로 가벼운 작업으로 처리되었습니다. 이는 해당 모델에 대해 GPU가 과도한 사양일 수 있음을 의미하며, 더 많은 병렬 처리나 더 복잡한 모델에서 GPU의 진가가 드러날 것입니다.

*   **선택 가이드:**
    *   **최고의 추론 속도와 처리량**이 필요하고 전력 소비에 제약이 없다면 **TensorRT(GPU)**가 최적의 선택입니다.
    *   **범용성 및 개발 편의성**을 고려하면서 GPU 가속을 원한다면 **ONNX Runtime(GPU)**이 좋은 대안이 될 수 있습니다.
    *   **저전력, 소형 폼팩터의 엣지 환경**에서 AI 추론을 구현해야 한다면 **NPU(Hailo-8L)**가 적합합니다. NPU는 전력 대비 성능(TOPS/W)에서 강점을 가집니다.

각 플랫폼은 서로 다른 목표와 사용 사례를 가지고 설계되었으므로, 특정 요구사항에 따라 적합한 플랫폼을 선택하는 것이 중요합니다.