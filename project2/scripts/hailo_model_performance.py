#!/usr/bin/env python3

import numpy as np
import csv
import time
import json
import os
import subprocess
import threading
import queue
import select
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
import re

from hailo_platform import (HEF, VDevice, HailoSchedulingAlgorithm, 
                           HailoStreamInterface, ConfigureParams,
                           InputVStreamParams, OutputVStreamParams,
                           FormatType, InferVStreams)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NPUPerformanceMeasurer:
    def __init__(self, models_dir: str, data_dir: str, results_dir: str = "results"):
        """
        NPU ì„±ëŠ¥ ì¸¡ì • í´ë˜ìŠ¤
        
        Args:
            models_dir: .hef íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            data_dir: CSV ë°ì´í„° íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬  
            results_dir: ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.models_dir = Path(models_dir)
        self.data_dir = Path(data_dir)
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # ì¢…ëª© ë° ëª¨ë¸ ì •ì˜
        self.stocks = ["Apple", "KOSPI", "NASDAQ", "Samsung", "Tesla", "SnP500"]
        self.model_types = ["GRU", "LSTM"]
        
        # ì¸¡ì • ì„¤ì •
        self.warmup_iterations = 10
        self.measurement_iterations = 50
        self.batch_size = 1
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = {}
        
    def preprocess_csv_data(self, csv_path: str, sequence_length: int = 8) -> np.ndarray:
        """
        CSV ë°ì´í„°ë¥¼ NPU ì…ë ¥ í˜•íƒœë¡œ ì „ì²˜ë¦¬
        
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’ 8)
            
        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„° ë°°ì—´
        """
        try:
            logger.info(f"Reading CSV file: {csv_path}")
            
            data_rows = []
            
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                
                # ì²˜ìŒ 3ì¤„ í—¤ë” ìŠ¤í‚µ
                headers1 = next(reader)  # Price,Open,High,Low,Close,Volume
                headers2 = next(reader)  # Ticker,AAPL,AAPL,AAPL,AAPL,AAPL
                headers3 = next(reader)  # Date,,,,,
                
                logger.info(f"CSV headers: {headers1}")
                logger.info(f"Skipped lines: {headers2}, {headers3}")
                
                row_count = 0
                for row in reader:
                    if row_count < 3:  # ì²˜ìŒ 3í–‰ ë¡œê·¸ë¡œ í™•ì¸
                        logger.info(f"Sample row {row_count}: {row}")
                    
                    try:
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì€ ë‚ ì§œì´ë¯€ë¡œ ìŠ¤í‚µí•˜ê³ , ë‚˜ë¨¸ì§€ 5ê°œ ì»¬ëŸ¼(OHLCV) ì‚¬ìš©
                        if len(row) >= 6:  # ë‚ ì§œ + OHLCV 5ê°œ
                            # ë‚ ì§œ ì»¬ëŸ¼(index 0) ìŠ¤í‚µí•˜ê³  OHLCV ë°ì´í„°(index 1-5) ì¶”ì¶œ
                            ohlcv_values = []
                            for i in range(1, 6):  # Open, High, Low, Close, Volume
                                if i < len(row):
                                    try:
                                        value = float(row[i].strip())
                                        ohlcv_values.append(value)
                                    except (ValueError, AttributeError):
                                        logger.warning(f"Cannot convert '{row[i]}' to float in row {row_count}")
                                        break
                                else:
                                    break
                            
                            # 5ê°œ ê°’ì´ ëª¨ë‘ ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆìœ¼ë©´ ì¶”ê°€
                            if len(ohlcv_values) == 5:
                                data_rows.append(ohlcv_values)
                            
                        row_count += 1
                        if row_count >= 2000:  # ì¶©ë¶„í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
                            break
                            
                    except Exception as e:
                        logger.warning(f"Error parsing row {row_count}: {e}")
                        continue
            
            if not data_rows:
                logger.error(f"No valid rows found in {csv_path}")
                # íŒŒì¼ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
                with open(csv_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()[:10]
                    for i, line in enumerate(lines):
                        logger.error(f"Line {i}: {line.strip()}")
                raise ValueError(f"No valid numeric data found in {csv_path}")
            
            logger.info(f"Successfully loaded {len(data_rows)} rows from {csv_path}")
            logger.info(f"Sample data (first row): {data_rows[0] if data_rows else 'None'}")
            logger.info(f"Sample data (last row): {data_rows[-1] if data_rows else 'None'}")
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            ohlcv_data = np.array(data_rows, dtype=np.float32)
            
            # ì •ê·œí™” (TODO: GPU í•™ìŠµì‹œ ì‚¬ìš©í•œ scaler íŒŒë¼ë¯¸í„° ì ìš© í•„ìš”)
            # ì„ì‹œë¡œ MinMax ì •ê·œí™” ì‚¬ìš© (0-1 ë²”ìœ„)
            data_min = np.min(ohlcv_data, axis=0, keepdims=True)
            data_max = np.max(ohlcv_data, axis=0, keepdims=True)
            
            logger.info(f"Data range - Min: {data_min.flatten()}, Max: {data_max.flatten()}")
            
            # Zero division ë°©ì§€
            data_range = data_max - data_min
            data_range[data_range == 0] = 1.0
            
            normalized_data = (ohlcv_data - data_min) / data_range
            
            # UINT8 ë³€í™˜ (0-255 ë²”ìœ„)
            uint8_data = (normalized_data * 255).astype(np.uint8)
            
            # ì‹œí€€ìŠ¤ ìƒì„± (sliding window)
            sequences = []
            for i in range(sequence_length, len(uint8_data)):
                seq = uint8_data[i-sequence_length:i]  # (8, 5)
                sequences.append(seq)
            
            logger.info(f"Generated {len(sequences)} sequences of shape (8, 5)")
            return np.array(sequences), data_min, data_max
            
        except Exception as e:
            logger.error(f"Error preprocessing {csv_path}: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None
    
    def create_gru_inputs(self, sequences: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        GRU ëª¨ë¸ìš© ì…ë ¥ ë°ì´í„° ìƒì„±
        
        Args:
            sequences: ì „ì²˜ë¦¬ëœ ì‹œí€€ìŠ¤ ë°ì´í„°
            
        Returns:
            (input_x, input_h0) íŠœí”Œ
        """
        # input_x: (1, 8, 5) -> (1, 1, 8, 5)
        input_x = sequences[0:1].reshape(1, 1, 8, 5)
        
        # input_h0: ì˜ë²¡í„° ì´ˆê¸°í™” (1, 1, 5)
        input_h0 = np.zeros((1, 1, 1, 5), dtype=np.uint8)
        
        return input_x, input_h0
    
    def create_lstm_inputs(self, sequences: np.ndarray) -> np.ndarray:
        """
        LSTM ëª¨ë¸ìš© ì…ë ¥ ë°ì´í„° ìƒì„±
        
        Args:
            sequences: ì „ì²˜ë¦¬ëœ ì‹œí€€ìŠ¤ ë°ì´í„°
            
        Returns:
            input_x ë°°ì—´
        """
        # input_x: (1, 8, 5) -> (1, 1, 8, 5)
        input_x = sequences[0:1].reshape(1, 1, 8, 5)
        return input_x
    
    def measure_latency(self, hef_path: str, input_data: Dict[str, np.ndarray]) -> Dict[str, float]:
        """
        ë ˆì´í„´ì‹œ ì¸¡ì •
        
        Args:
            hef_path: HEF íŒŒì¼ ê²½ë¡œ
            input_data: ì…ë ¥ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ë ˆì´í„´ì‹œ í†µê³„
        """
        try:
            hef = HEF(hef_path)
            
            # VDevice ìƒì„±
            params = VDevice.create_params()
            params.scheduling_algorithm = HailoSchedulingAlgorithm.ROUND_ROBIN
            
            with VDevice(params) as vdevice:
                # ëª¨ë¸ êµ¬ì„±
                infer_model = vdevice.create_infer_model(hef_path)
                infer_model.set_batch_size(self.batch_size)
                
                with infer_model.configure() as configured_model:
                    # Bindings ìƒì„±
                    bindings = configured_model.create_bindings()
                    
                    # ì…ë ¥ ë²„í¼ ì„¤ì •
                    for input_name in infer_model.input_names:
                        if input_name in input_data:
                            bindings.input(input_name).set_buffer(input_data[input_name])
                        else:
                            logger.warning(f"Input {input_name} not found in input_data")
                    
                    # ì¶œë ¥ ë²„í¼ ì„¤ì •
                    for output_name in infer_model.output_names:
                        output_stream = infer_model.output(output_name)
                        shape = output_stream.shape
                        buffer = np.empty(shape, dtype=np.uint8)
                        bindings.output(output_name).set_buffer(buffer)
                    
                    # ë ˆì´í„´ì‹œ ì¸¡ì •
                    latencies = []
                    timeout_ms = 1000
                    
                    # Warmup
                    logger.info(f"Warming up with {self.warmup_iterations} iterations...")
                    for _ in range(self.warmup_iterations):
                        configured_model.run([bindings], timeout_ms)
                    
                    # ì‹¤ì œ ì¸¡ì •
                    logger.info(f"Measuring latency over {self.measurement_iterations} iterations...")
                    for i in range(self.measurement_iterations):
                        start = time.perf_counter()
                        configured_model.run([bindings], timeout_ms)
                        end = time.perf_counter()
                        
                        latencies.append((end - start) * 1000)  # ms ë³€í™˜
                        
                        if (i + 1) % 10 == 0:
                            logger.info(f"Progress: {i + 1}/{self.measurement_iterations}")
                    
                    # í†µê³„ ê³„ì‚°
                    latencies = np.array(latencies)
                    return {
                        "mean": float(np.mean(latencies)),
                        "median": float(np.median(latencies)),
                        "min": float(np.min(latencies)),
                        "max": float(np.max(latencies)),
                        "std": float(np.std(latencies)),
                        "p95": float(np.percentile(latencies, 95)),
                        "p99": float(np.percentile(latencies, 99))
                    }
        
        except Exception as e:
            logger.error(f"Error measuring latency for {hef_path}: {e}")
            return None
    
    def measure_throughput_infer_model(self, hef_path: str, input_data: Dict[str, np.ndarray], 
                                      duration: int = 10) -> Optional[float]:
        """
        InferModel APIë¥¼ ì‚¬ìš©í•œ ìŠ¤ë£¨í’‹(FPS) ì¸¡ì •
        
        Args:
            hef_path: HEF íŒŒì¼ ê²½ë¡œ
            input_data: ì…ë ¥ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            duration: ì¸¡ì • ì§€ì† ì‹œê°„ (ì´ˆ)
            
        Returns:
            FPS ê°’
        """
        try:
            hef = HEF(hef_path)
            
            # VDevice ìƒì„±
            params = VDevice.create_params()
            params.scheduling_algorithm = HailoSchedulingAlgorithm.ROUND_ROBIN
            
            with VDevice(params) as vdevice:
                # ëª¨ë¸ êµ¬ì„±
                infer_model = vdevice.create_infer_model(hef_path)
                infer_model.set_batch_size(self.batch_size)
                
                with infer_model.configure() as configured_model:
                    # Bindings ìƒì„±
                    bindings = configured_model.create_bindings()
                    
                    # ì…ë ¥ ë²„í¼ ì„¤ì •
                    for input_name in infer_model.input_names:
                        if input_name in input_data:
                            bindings.input(input_name).set_buffer(input_data[input_name])
                    
                    # ì¶œë ¥ ë²„í¼ ì„¤ì •
                    for output_name in infer_model.output_names:
                        output_stream = infer_model.output(output_name)
                        shape = output_stream.shape
                        buffer = np.empty(shape, dtype=np.uint8)
                        bindings.output(output_name).set_buffer(buffer)
                    
                    # Throughput ì¸¡ì •
                    timeout_ms = 1000
                    
                    # Warmup
                    logger.info("Warming up for throughput measurement...")
                    for _ in range(20):
                        configured_model.run([bindings], timeout_ms)
                    
                    # ì‹¤ì œ ì¸¡ì •
                    logger.info(f"Measuring throughput for {duration} seconds...")
                    start_time = time.time()
                    inference_count = 0
                    
                    while time.time() - start_time < duration:
                        configured_model.run([bindings], timeout_ms)
                        inference_count += 1
                        
                        # 1ì´ˆë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                        elapsed = time.time() - start_time
                        if inference_count % 100 == 0:
                            current_fps = inference_count / elapsed
                            logger.info(f"  {elapsed:.1f}s: {current_fps:.1f} FPS")
                    
                    total_time = time.time() - start_time
                    fps = inference_count / total_time
                    
                    logger.info(f"Completed {inference_count} inferences in {total_time:.2f}s")
                    return fps
        
        except Exception as e:
            logger.error(f"Error measuring throughput for {hef_path}: {e}")
            return None
    
    def start_memory_monitor(self, duration: int = 60) -> queue.Queue:
        """
        ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            duration: ëª¨ë‹ˆí„°ë§ ì§€ì† ì‹œê°„ (ì´ˆ)
            
        Returns:
            ëª¨ë‹ˆí„°ë§ ê²°ê³¼ë¥¼ ë‹´ì„ í
        """
        monitor_queue = queue.Queue()
        
        def monitor_thread():
            try:
                # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ ë³µì‚¬ + HAILO_MONITOR ì¶”ê°€)
                env = os.environ.copy()
                env['HAILO_MONITOR'] = '1'
                env['HAILO_MONITOR_TIME_INTERVAL'] = '500'  # 500ms ê°„ê²©
                
                logger.info("Starting hailortcli monitor...")
                
                # hailortcli monitor ì‹¤í–‰
                cmd = ["hailortcli", "monitor"]
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    env=env,
                    universal_newlines=True,
                    bufsize=1  # ë¼ì¸ ë²„í¼ë§
                )
                
                start_time = time.time()
                output_lines = []
                error_lines = []
                
                logger.info(f"Monitor started, PID: {process.pid}")
                
                # Non-blocking ì½ê¸°ë¡œ ë³€ê²½
                while time.time() - start_time < duration:
                    try:
                        # stdoutì—ì„œ ì½ê¸° (timeout 1ì´ˆ)
                        ready_stdout, _, _ = select.select([process.stdout], [], [], 1.0)
                        
                        if ready_stdout:
                            line = process.stdout.readline()
                            if line:
                                line = line.strip()
                                if line:  # ë¹ˆ ì¤„ ì œì™¸
                                    output_lines.append(line)
                                    logger.debug(f"Monitor output: {line}")
                        
                        # stderr í™•ì¸
                        ready_stderr, _, _ = select.select([process.stderr], [], [], 0.1)
                        if ready_stderr:
                            error_line = process.stderr.readline()
                            if error_line:
                                error_lines.append(error_line.strip())
                                logger.warning(f"Monitor error: {error_line.strip()}")
                        
                        # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ í™•ì¸
                        if process.poll() is not None:
                            logger.warning(f"Monitor process terminated early with code: {process.poll()}")
                            break
                            
                    except Exception as e:
                        logger.error(f"Error reading monitor output: {e}")
                        break
                
                # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
                try:
                    process.terminate()
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    logger.warning("Monitor process did not terminate, killing...")
                    process.kill()
                    process.wait()
                
                logger.info(f"Monitor finished. Captured {len(output_lines)} output lines, {len(error_lines)} error lines")
                
                if error_lines:
                    logger.error(f"Monitor errors: {error_lines[:5]}")  # ì²˜ìŒ 5ê°œ ì—ëŸ¬ë§Œ ë¡œê·¸
                
                monitor_queue.put({
                    'output': output_lines,
                    'errors': error_lines,
                    'success': len(output_lines) > 0
                })
                
            except Exception as e:
                logger.error(f"Memory monitoring thread error: {e}")
                import traceback
                traceback.print_exc()
                monitor_queue.put({
                    'output': [],
                    'errors': [str(e)],
                    'success': False
                })
        
        thread = threading.Thread(target=monitor_thread, daemon=True)
        thread.start()
        
        return monitor_queue
    
    def parse_monitor_output(self, monitor_data: Dict) -> Dict[str, float]:
        """
        ëª¨ë‹ˆí„° ì¶œë ¥ íŒŒì‹± (í…Œì´ë¸” êµ¬ì¡° ê¸°ë°˜)
        
        Args:
            monitor_data: ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            íŒŒì‹±ëœ ë©”ëª¨ë¦¬/ì„±ëŠ¥ ì •ë³´
        """
        if not monitor_data.get('success', False):
            logger.warning(f"Monitor failed: {monitor_data.get('errors', [])}")
            return {
                "monitor_success": False,
                "error_message": str(monitor_data.get('errors', ['Unknown error'])[0]) if monitor_data.get('errors') else "No data"
            }
        
        monitor_lines = monitor_data.get('output', [])
        device_utilization = []
        model_fps = []
        model_utilization = []
        
        logger.info(f"Parsing {len(monitor_lines)} monitor lines...")
        
        # ìƒ˜í”Œ ë¡œê·¸ ì¶œë ¥ (ì²˜ìŒ 10ì¤„)
        for i, line in enumerate(monitor_lines[:10]):
            logger.info(f"Monitor line {i}: {line}")
        
        parsing_device_table = False
        parsing_model_table = False
        
        for line in monitor_lines:
            line = line.strip()
            
            # í…Œì´ë¸” ì„¹ì…˜ ê°ì§€
            if "Device ID" in line and "Utilization" in line:
                parsing_device_table = True
                parsing_model_table = False
                continue
            elif "Model" in line and "Utilization" in line and "FPS" in line:
                parsing_device_table = False
                parsing_model_table = True
                continue
            elif "Stream" in line and "Direction" in line:
                parsing_device_table = False
                parsing_model_table = False
                continue
            elif line.startswith("---"):
                continue
            
            # Device í…Œì´ë¸” íŒŒì‹±
            if parsing_device_table and line and not line.startswith("Device"):
                # í˜•íƒœ: "0000:01:00.0                                                75.1                     HAILO8L"
                parts = line.split()
                if len(parts) >= 3:
                    try:
                        # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì´ utilization
                        util = float(parts[1])
                        if 0 <= util <= 100:
                            device_utilization.append(util)
                            logger.debug(f"Device utilization: {util}%")
                    except (ValueError, IndexError):
                        continue
            
            # Model í…Œì´ë¸” íŒŒì‹±  
            elif parsing_model_table and line and not line.startswith("Model"):
                # í˜•íƒœ: "GRU_Apple                                                   75.1                     765.4          4823"
                parts = line.split()
                if len(parts) >= 4:
                    try:
                        # ë‘ ë²ˆì§¸ ì»¬ëŸ¼: Model utilization, ì„¸ ë²ˆì§¸ ì»¬ëŸ¼: FPS
                        model_util = float(parts[1])
                        fps = float(parts[2])
                        
                        if 0 <= model_util <= 100:
                            model_utilization.append(model_util)
                            logger.debug(f"Model utilization: {model_util}%")
                        
                        if 0 < fps < 10000:
                            model_fps.append(fps)
                            logger.debug(f"Model FPS: {fps}")
                            
                    except (ValueError, IndexError):
                        continue
            
            # ì¶”ê°€: ì •ê·œì‹ìœ¼ë¡œ ìˆ«ìë§Œ ì°¾ê¸° (ë°±ì—… ë°©ë²•)
            else:
                # ë°±ë¶„ìœ¨ ì°¾ê¸°
                percentages = re.findall(r'(\d+\.?\d*)\s*%', line)
                for p in percentages:
                    try:
                        util = float(p)
                        if 0 <= util <= 100:
                            if "device" in line.lower() or "0000:" in line:
                                device_utilization.append(util)
                            else:
                                model_utilization.append(util)
                    except ValueError:
                        continue
                
                # FPS íŒ¨í„´ ì°¾ê¸° (ìˆ«ì ë’¤ì— ê³µë°±ì´ë‚˜ íƒ­ì´ ìˆëŠ” ê²½ìš°)
                fps_matches = re.findall(r'(\d+\.?\d+)\s+\d+', line)  # ìˆ«ì ë’¤ì— PIDê°€ ì˜¤ëŠ” íŒ¨í„´
                for fps_str in fps_matches:
                    try:
                        fps = float(fps_str)
                        if 0 < fps < 10000:
                            model_fps.append(fps)
                    except ValueError:
                        continue
        
        # í†µê³„ ê³„ì‚°
        result = {
            "monitor_success": True,
            "total_lines": len(monitor_lines),
            
            # Device í†µê³„
            "avg_device_utilization": float(np.mean(device_utilization)) if device_utilization else 0.0,
            "max_device_utilization": float(np.max(device_utilization)) if device_utilization else 0.0,
            "min_device_utilization": float(np.min(device_utilization)) if device_utilization else 0.0,
            "device_util_samples": len(device_utilization),
            "device_util_values": device_utilization[:10],  # ì²˜ìŒ 10ê°œ ê°’ ì €ì¥
            
            # Model FPS í†µê³„
            "avg_model_fps": float(np.mean(model_fps)) if model_fps else 0.0,
            "max_model_fps": float(np.max(model_fps)) if model_fps else 0.0,
            "min_model_fps": float(np.min(model_fps)) if model_fps else 0.0,
            "fps_samples": len(model_fps),
            "fps_values": model_fps[:10],  # ì²˜ìŒ 10ê°œ ê°’ ì €ì¥
            
            # Model Utilization í†µê³„
            "avg_model_utilization": float(np.mean(model_utilization)) if model_utilization else 0.0,
            "max_model_utilization": float(np.max(model_utilization)) if model_utilization else 0.0,
            "min_model_utilization": float(np.min(model_utilization)) if model_utilization else 0.0,
            "utilization_samples": len(model_utilization),
            "utilization_values": model_utilization[:10]  # ì²˜ìŒ 10ê°œ ê°’ ì €ì¥
        }
        
        logger.info(f"Parsed monitor data successfully:")
        logger.info(f"  Device utilization samples: {len(device_utilization)} (avg: {result['avg_device_utilization']:.1f}%)")
        logger.info(f"  Model FPS samples: {len(model_fps)} (avg: {result['avg_model_fps']:.1f})")
        logger.info(f"  Model utilization samples: {len(model_utilization)} (avg: {result['avg_model_utilization']:.1f}%)")
        
        return result
    
    def calculate_rmse(self, predictions: np.ndarray, actual_data: np.ndarray, 
                      data_min: np.ndarray, data_max: np.ndarray) -> float:
        """
        RMSE ê³„ì‚°
        
        Args:
            predictions: NPU ì˜ˆì¸¡ ê²°ê³¼
            actual_data: ì‹¤ì œ ë°ì´í„°
            data_min, data_max: ì •ê·œí™” íŒŒë¼ë¯¸í„°
            
        Returns:
            RMSE ê°’
        """
        try:
            # UINT8 -> ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ë³€í™˜
            pred_normalized = predictions.astype(np.float32) / 255.0
            
            # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
            pred_original = pred_normalized * (data_max - data_min) + data_min
            
            # ì‹¤ì œê°’ê³¼ ë¹„êµ (Close ê°€ê²©ë§Œ ë¹„êµ)
            actual_close = actual_data[:len(pred_original), 3]  # CloseëŠ” 4ë²ˆì§¸ ì»¬ëŸ¼
            pred_close = pred_original[:, 3] if pred_original.ndim > 1 else pred_original
            
            # RMSE ê³„ì‚°
            mse = np.mean((actual_close - pred_close) ** 2)
            rmse = np.sqrt(mse)
            
            return float(rmse)
            
        except Exception as e:
            logger.error(f"Error calculating RMSE: {e}")
            return None
    
    def measure_single_model(self, stock: str, model_type: str) -> Dict:
        """
        ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        
        Args:
            stock: ì¢…ëª©ëª…
            model_type: ëª¨ë¸ íƒ€ì… (GRU/LSTM)
            
        Returns:
            ì¸¡ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"Measuring {model_type}_{stock}...")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        hef_path = self.models_dir / f"{model_type}_{stock}.hef"
        csv_path = self.data_dir / f"{stock}_actual_ohlcv.csv"
        
        if not hef_path.exists():
            logger.error(f"HEF file not found: {hef_path}")
            return None
        
        if not csv_path.exists():
            logger.error(f"CSV file not found: {csv_path}")
            return None
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        sequences, data_min, data_max = self.preprocess_csv_data(str(csv_path))
        if sequences is None:
            return None
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        if model_type == "GRU":
            input_x, input_h0 = self.create_gru_inputs(sequences)
            input_data = {
                f"{model_type}_{stock}/input_layer1": input_x,
                f"{model_type}_{stock}/input_layer2": input_h0
            }
        else:  # LSTM
            input_x = self.create_lstm_inputs(sequences)
            input_data = {f"{model_type}_{stock}/input_layer1": input_x}
        
        # 1. ë ˆì´í„´ì‹œ ì¸¡ì •
        logger.info("=== Measuring Latency ===")
        latency_stats = self.measure_latency(str(hef_path), input_data)
        
        # 2. ìŠ¤ë£¨í’‹ ì¸¡ì • (ìƒˆë¡œìš´ ë°©ì‹)
        logger.info("=== Measuring Throughput ===")
        throughput = self.measure_throughput_infer_model(str(hef_path), input_data, duration=10)
        
        # 3. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (ì¶”ë¡ ê³¼ ë™ì‹œì—)
        logger.info("=== Measuring Memory Usage ===")
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor_queue = self.start_memory_monitor(duration=15)
        time.sleep(1)  # ëª¨ë‹ˆí„° ì‹œì‘ ëŒ€ê¸°
        
        # ì¶”ë¡ ì„ ì•½ê°„ ì‹¤í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        try:
            hef = HEF(str(hef_path))
            params = VDevice.create_params()
            params.scheduling_algorithm = HailoSchedulingAlgorithm.ROUND_ROBIN
            
            with VDevice(params) as vdevice:
                infer_model = vdevice.create_infer_model(str(hef_path))
                infer_model.set_batch_size(self.batch_size)
                
                with infer_model.configure() as configured_model:
                    bindings = configured_model.create_bindings()
                    
                    # ì…ë ¥ ë²„í¼ ì„¤ì •
                    for input_name in infer_model.input_names:
                        if input_name in input_data:
                            bindings.input(input_name).set_buffer(input_data[input_name])
                    
                    # ì¶œë ¥ ë²„í¼ ì„¤ì •
                    for output_name in infer_model.output_names:
                        output_stream = infer_model.output(output_name)
                        shape = output_stream.shape
                        buffer = np.empty(shape, dtype=np.uint8)
                        bindings.output(output_name).set_buffer(buffer)
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •ì„ ìœ„í•´ ì¶”ë¡  ì‹¤í–‰
                    logger.info("Running inference for memory monitoring...")
                    for i in range(100):
                        configured_model.run([bindings], 1000)
                        if i % 20 == 0:
                            logger.info(f"  Memory test progress: {i}/100")
                        time.sleep(0.05)  # ëª¨ë‹ˆí„°ë§ ì‹œê°„ í™•ë³´
                    
        except Exception as e:
            logger.error(f"Error during memory monitoring: {e}")
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ìˆ˜ì§‘
        time.sleep(3)  # ëª¨ë‹ˆí„°ë§ ì™„ë£Œ ëŒ€ê¸°
        try:
            monitor_data = monitor_queue.get(timeout=10)
            memory_stats = self.parse_monitor_output(monitor_data)
            logger.info(f"Memory monitoring success: {monitor_data.get('success', False)}")
        except queue.Empty:
            logger.warning("Memory monitoring queue timeout")
            memory_stats = {"monitor_success": False, "error_message": "Queue timeout"}
        
        # RMSE ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        # TODO: ì‹¤ì œ NPU ì¶”ë¡  ê²°ê³¼ë¡œ RMSE ê³„ì‚°
        rmse = None  # ì¶”í›„ êµ¬í˜„
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            "model": f"{model_type}_{stock}",
            "timestamp": datetime.now().isoformat(),
            "latency": latency_stats,
            "throughput_fps": throughput,
            "memory": memory_stats,
            "rmse": rmse,
            "data_samples": len(sequences)
        }
        
        logger.info(f"=== Results for {model_type}_{stock} ===")
        if latency_stats:
            logger.info(f"  Latency: {latency_stats['mean']:.2f}ms (Â±{latency_stats['std']:.2f})")
        if throughput:
            logger.info(f"  Throughput: {throughput:.1f} FPS")
        if memory_stats:
            logger.info(f"  Memory: {memory_stats}")
        
        return result
    
    def run_all_measurements(self):
        """
        ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ì„±ëŠ¥ ì¸¡ì • ì‹¤í–‰
        """
        logger.info("Starting NPU performance measurements...")
        
        total_models = len(self.stocks) * len(self.model_types)
        current = 0
        
        for stock in self.stocks:
            for model_type in self.model_types:
                current += 1
                logger.info(f"Progress: {current}/{total_models}")
                
                result = self.measure_single_model(stock, model_type)
                if result:
                    self.results[f"{model_type}_{stock}"] = result
                
                # ëª¨ë¸ ê°„ ì¿¨ë‹¤ìš´
                time.sleep(3)
        
        # ê²°ê³¼ ì €ì¥
        self.save_results()
        logger.info("All measurements completed!")
    
    def save_results(self):
        """
        ì¸¡ì • ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.results_dir / f"npu_performance_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results saved to {results_file}")
        
        # ìš”ì•½ ì¶œë ¥
        self.print_summary()
    
    def print_summary(self):
        """
        ì¸¡ì • ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        """
        print("\n" + "="*70)
        print("NPU PERFORMANCE MEASUREMENT SUMMARY")
        print("="*70)
        
        gru_results = {}
        lstm_results = {}
        
        # ê²°ê³¼ ë¶„ë¥˜
        for model_name, result in self.results.items():
            if result and 'GRU' in model_name:
                gru_results[model_name] = result
            elif result and 'LSTM' in model_name:
                lstm_results[model_name] = result
        
        # GRU ê²°ê³¼
        print("\nğŸ”¸ GRU Models:")
        for model_name, result in gru_results.items():
            print(f"\n  {model_name}:")
            if result.get('latency'):
                lat = result['latency']
                print(f"    Latency: {lat['mean']:.2f}ms (Â±{lat['std']:.2f}ms)")
                print(f"    Range: {lat['min']:.2f} - {lat['max']:.2f}ms")
            if result.get('throughput_fps'):
                print(f"    Throughput: {result['throughput_fps']:.1f} FPS")
            if result.get('memory', {}).get('monitor_success'):
                mem = result['memory']
                if mem.get('avg_device_utilization', 0) > 0:
                    print(f"    Device Util: {mem['avg_device_utilization']:.1f}% (max: {mem['max_device_utilization']:.1f}%)")
                if mem.get('avg_model_fps', 0) > 0:
                    print(f"    Monitor FPS: {mem['avg_model_fps']:.1f}")
            elif result.get('memory', {}).get('error_message'):
                print(f"    Memory: {result['memory']['error_message']}")
        
        # LSTM ê²°ê³¼
        print("\nğŸ”¹ LSTM Models:")
        for model_name, result in lstm_results.items():
            print(f"\n  {model_name}:")
            if result.get('latency'):
                lat = result['latency']
                print(f"    Latency: {lat['mean']:.2f}ms (Â±{lat['std']:.2f}ms)")
                print(f"    Range: {lat['min']:.2f} - {lat['max']:.2f}ms")
            if result.get('throughput_fps'):
                print(f"    Throughput: {result['throughput_fps']:.1f} FPS")
            if result.get('memory', {}).get('monitor_success'):
                mem = result['memory']
                if mem.get('avg_device_utilization', 0) > 0:
                    print(f"    Device Util: {mem['avg_device_utilization']:.1f}% (max: {mem['max_device_utilization']:.1f}%)")
                if mem.get('avg_model_fps', 0) > 0:
                    print(f"    Monitor FPS: {mem['avg_model_fps']:.1f}")
            elif result.get('memory', {}).get('error_message'):
                print(f"    Memory: {result['memory']['error_message']}")
        
        # ì „ì²´ í†µê³„
        print("\nğŸ“Š Overall Statistics:")
        all_latencies = []
        all_throughputs = []
        
        for result in self.results.values():
            if result and result.get('latency'):
                all_latencies.append(result['latency']['mean'])
            if result and result.get('throughput_fps'):
                all_throughputs.append(result['throughput_fps'])
        
        if all_latencies:
            print(f"  Average Latency: {np.mean(all_latencies):.2f}ms")
            print(f"  Latency Range: {np.min(all_latencies):.2f} - {np.max(all_latencies):.2f}ms")
        
        if all_throughputs:
            print(f"  Average Throughput: {np.mean(all_throughputs):.1f} FPS")
            print(f"  Throughput Range: {np.min(all_throughputs):.1f} - {np.max(all_throughputs):.1f} FPS")
        
        print(f"\n  Total Models Tested: {len(self.results)}")
        print("="*70)


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ['HAILO_MONITOR'] = '1'
    os.environ['HAILO_MONITOR_TIME_INTERVAL'] = '500'  # 500ms ê°„ê²©
    
    # ê²½ë¡œ ì„¤ì • (ì •í™•í•œ ê²½ë¡œë¡œ ìˆ˜ì •)
    models_dir = "/root/2025-05-25/compiled_model/hef"  # .hef íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
    data_dir = "/root/2025-05-25/data"  # CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
    
    # ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(models_dir):
        print(f"Models directory not found: {models_dir}")
        return
    
    if not os.path.exists(data_dir):
        print(f"Data directory not found: {data_dir}")
        return
    
    print(f"Models directory: {models_dir}")
    print(f"Data directory: {data_dir}")
    print(f"HAILO_MONITOR environment variable set: {os.environ.get('HAILO_MONITOR')}")
    
    # ì„±ëŠ¥ ì¸¡ì • ì‹¤í–‰
    measurer = NPUPerformanceMeasurer(models_dir, data_dir)
    measurer.run_all_measurements()


if __name__ == "__main__":
    main()